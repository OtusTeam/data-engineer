{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark context started\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('twitter-sentiment').getOrCreate()\n",
    "spark.sparkContext.addPyFile(\"/home/jovyan/work/sentiment_model.py\")\n",
    "print(\"Spark context started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addPyFile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|target| count|\n",
      "+------+------+\n",
      "|     1|800000|\n",
      "|     0|800000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"target\", IntegerType(), True),\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"raw_timestamp\", StringType(), True),\n",
    "    StructField(\"query_status\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"tweet\", StringType(), True)\n",
    "])\n",
    "    \n",
    "data_path = \"/home/jovyan/data/training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "raw_sentiment = spark.read.csv(data_path,header=False,schema=schema) \\\n",
    "    .selectExpr(\"(case when target=4 then 1 else 0 end) as target\",\"tweet\")\n",
    "\n",
    "\n",
    "\n",
    "raw_sentiment.groupBy(\"target\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=0.75, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=<function preprocessor at 0x7fb955a06ef0>,\n",
       "                                 smooth_idf=True, stop_words=None,\n",
       "                                 strip_accents=None...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=8, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "def read_model(model_path):\n",
    "    with open(model_path,'rb') as buffer:\n",
    "        return pkl.load(buffer)\n",
    "\n",
    "model_path = \"/home/jovyan/models/tweet_sentiment.mdl\"\n",
    "\n",
    "model_object = read_model(model_path)\n",
    "model_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- target: integer (nullable = false)\n",
      " |-- tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# raw_sentiment.summary()\n",
    "# raw_sentiment.columns\n",
    "# raw_sentiment.dtypes\n",
    "raw_sentiment.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_sentiment.rdd?\n",
    "raw_sentiment.rdd.mapPartitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_object_broadcast = spark.sparkContext.broadcast(model_object)\n",
    "# model_object_broadcast?\n",
    "# model_object_broadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=0.75, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=<function preprocessor at 0x7fb955a06ef0>,\n",
       "                                 smooth_idf=True, stop_words=None,\n",
       "                                 strip_accents=None...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=8, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_object_broadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/session.py:366: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+--------------------+\n",
      "|       proba|target|                text|\n",
      "+------------+------+--------------------+\n",
      "|0.5172669619|     0|@switchfoot http:...|\n",
      "|0.4887238607|     0|is upset that he ...|\n",
      "|0.4964520864|     0|@Kenichan I dived...|\n",
      "|0.4913687932|     0|my whole body fee...|\n",
      "|0.4807232839|     0|@nationwideclass ...|\n",
      "|0.5034112995|     0|@Kwesidei not the...|\n",
      "|0.4965544755|     0|         Need a hug |\n",
      "|0.4956864947|     0|@LOLTrish hey  lo...|\n",
      "|0.4838413254|     0|@Tatiana_K nope t...|\n",
      "|0.5034112995|     0|@twittera que me ...|\n",
      "|0.5065598329|     0|spring break in p...|\n",
      "|0.5034112995|     0|I just re-pierced...|\n",
      "|0.4929139023|     0|@caregiving I cou...|\n",
      "|0.4755583492|     0|@octolinz16 It it...|\n",
      "|0.4844650004|     0|@smarrison i woul...|\n",
      "|0.4688346285|     0|@iamjazzyfizzle I...|\n",
      "|0.4845203707|     0|Hollis' death sce...|\n",
      "|0.5034112995|     0|about to file taxes |\n",
      "|0.5160874226|     0|@LettyA ahh ive a...|\n",
      "|0.5034112995|     0|@FakerPattyPattz ...|\n",
      "+------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_object_broadcast = spark.sparkContext.broadcast(model_object)\n",
    "\n",
    "def block_iterator(iterator, size):\n",
    "    bucket = list()\n",
    "    for e in iterator:\n",
    "        bucket.append(e)\n",
    "        if len(bucket) >= size:\n",
    "            yield bucket\n",
    "            bucket = list()\n",
    "    if bucket:\n",
    "        yield bucket\n",
    "\n",
    "def block_classify(iterator):\n",
    "    model = model_object_broadcast.value\n",
    "    for features in block_iterator(iterator, 10000):\n",
    "        import pandas as pd\n",
    "        import json\n",
    "        features_df = pd.DataFrame(features, columns=[\"target\",\"text\"])\n",
    "        pred = model.predict_proba(features_df[\"text\"])\n",
    "        res_df = features_df\n",
    "        res_df[\"proba\"] = pred[:,1]\n",
    "        for e in json.loads(res_df.to_json(orient='records')):\n",
    "            yield e\n",
    "\n",
    "predicted_df = raw_sentiment.rdd.mapPartitions(block_classify).toDF()\n",
    "\n",
    "predicted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_df?\n",
    "# predicted_df.columns\n",
    "# predicted_df.dtypes\n",
    "# predicted_df.printSchema()\n",
    "print((predicted_df.count(), len(predicted_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
